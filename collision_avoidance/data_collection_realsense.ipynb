{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"data_collection_realsense.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"7MUbEfKk7aLX","colab_type":"text"},"source":["# Collision Avoidance - Data Collection\n","\n","If you ran through the basic motion notebook, hopefully you're enjoying how easy it can be to make your Jetbot move around! Thats very cool!  But what's even cooler, is making JetBot move around all by itself!  \n","\n","This is a super hard task, that has many different approaches but the whole problem is usually broken down into easier sub-problems.  It could be argued that one of the most\n","important sub-problems to solve, is the problem of preventing the robot from entering dangerous situations!  We're calling this *collision avoidance*. \n","\n","In this set of notebooks, we're going to attempt to solve the problem using deep learning and a single, very versatile, sensor: the camera.  You'll see how with a neural network, camera, and the NVIDIA Jetson Nano, we can teach the robot a very useful behavior!\n","\n","The approach we take to avoiding collisions is to create a virtual \"safety bubble\" around the robot.  Within this safety bubble, the robot is able to spin in a circle without hitting any objects (or other dangerous situations like falling off a ledge).  \n","\n","\n","Of course, the robot is limited by what's in it's field of vision, and we can't prevent objects from being placed behind the robot, etc.  But we can prevent the robot from entering these scenarios itself.\n","\n","The way we'll do this is super simple:  \n","\n","First, we'll manually place the robot in scenarios where it's \"safety bubble\" is violated, and label these scenarios ``blocked``.  We save a snapshot of what the robot sees along with this label.\n","\n","Second, we'll manually place the robot in scenarios where it's safe to move forward a bit, and label these scenarios ``free``.  Likewise, we save a snapshot along with this label.\n","\n","That's all that we'll do in this notebook; data collection.  Once we have lots of images and labels, we'll upload this data to a GPU enabled machine where we'll *train* a neural network to predict whether the robot's safety bubble is being violated based off of the image it sees.  We'll use this to implement a simple collision avoidance behavior in the end :)\n","\n","> IMPORTANT NOTE:  When JetBot spins in place, it actually spins about the center between the two wheels, not the center of the robot chassis itself.  This is an important detail to remember when you're trying to estimate whether the robot's safety bubble is violated or not.  But don't worry, you don't have to be exact. If in doubt it's better to lean on the cautious side (a big safety bubble).  We want to make sure JetBot doesn't enter a scenario that it couldn't get out of by turning in place."]},{"cell_type":"code","metadata":{"id":"GqXECzcl7aLa","colab_type":"code","colab":{}},"source":["import os\n","import cv2\n","import sys\n","sys.path.append(\"/usr/local/lib\")\n","import IPython\n","import traitlets\n","import threading\n","import numpy as np\n","import pyrealsense2 as rs\n","import ipywidgets.widgets as widgets\n","from uuid import uuid1\n","from IPython.display import display"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"btc92dQL7aLf","colab_type":"code","colab":{},"outputId":"39f094c8-e8cf-4af8-e7ef-3759b85f38a2"},"source":["blocked_dir = 'dataset_0512/blocked'\n","free_dir = 'dataset_0512/free'\n","\n","# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n","try:\n","    os.makedirs(free_dir)\n","    os.makedirs(blocked_dir)\n","except FileExistsError:\n","    print('Directories not created becasue they already exist')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Directories not created becasue they already exist\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gjfgwzph7aLl","colab_type":"code","colab":{}},"source":["button_layout = widgets.Layout(width='128px', height='64px')\n","free_button = widgets.Button(description='add free', button_style='success', layout=button_layout)\n","blocked_button = widgets.Button(description='add blocked', button_style='danger', layout=button_layout)\n","free_count = widgets.IntText(layout=button_layout, value=len(os.listdir(free_dir)))\n","blocked_count = widgets.IntText(layout=button_layout, value=len(os.listdir(blocked_dir)))\n","image = widgets.Image(format='jpeg', width=1280, height=480) \n","out = widgets.Output()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvjqVXcz7aLp","colab_type":"code","colab":{"referenced_widgets":["06bb85929a6f4cf697fa3c94c6b55b1f","21318a4fa6b2460c849584f6aa5675a3","1c584ffbde7e4410a50ee0ba530d130e","ec7ee8c3a07d4830ac6fe6ceb9791896"]},"outputId":"b309fd0e-632b-46d0-a3fd-afe9b780e28e"},"source":["def save_snapshot(directory):\n","    image_path = os.path.join(directory, str(uuid1()) + '.jpg')\n","    with open(image_path, 'wb') as f:\n","        f.write(image.value)\n","        with out:\n","            print(type(image.value))\n","            print(image_path)\n","\n","def save_free():\n","    global free_dir, free_count\n","    save_snapshot(free_dir)\n","    free_count.value = len(os.listdir(free_dir))\n","    with out:\n","        print(\"free\")\n","    \n","def save_blocked():\n","    global blocked_dir, blocked_count\n","    save_snapshot(blocked_dir)\n","    blocked_count.value = len(os.listdir(blocked_dir))\n","    with out:\n","        print(\"blocked\")\n","    \n","free_button.on_click(lambda x: save_free())\n","blocked_button.on_click(lambda x: save_blocked())\n","\n","display(image)\n","display(widgets.HBox([free_count, free_button]))\n","display(widgets.HBox([blocked_count, blocked_button]))\n","display(out)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06bb85929a6f4cf697fa3c94c6b55b1f","version_major":2,"version_minor":0},"text/plain":["Image(value=b'', format='jpeg', height='480', width='1280')"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21318a4fa6b2460c849584f6aa5675a3","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntText(value=5, layout=Layout(height='64px', width='128px')), Button(button_style='success', d…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c584ffbde7e4410a50ee0ba530d130e","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntText(value=3, layout=Layout(height='64px', width='128px')), Button(button_style='danger', de…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec7ee8c3a07d4830ac6fe6ceb9791896","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"N3xPP0y-7aLu","colab_type":"text"},"source":["### Display live camera feed\n","\n","So let's get started.  First, let's initialize and display our camera like we did in the *teleoperation* notebook.  \n","\n","> Our neural network takes a 224x224 pixel image as input.  We'll set our camera to that size to minimize the filesize of our dataset (we've tested that it works for this task).\n","> In some scenarios it may be better to collect data in a larger image size and downscale to the desired size later."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SI3upKq-7aLu","colab_type":"code","colab":{}},"source":["def rs_on():\n","    pipeline = rs.pipeline()\n","    config = rs.config()\n","    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 6)\n","    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 6)\n","    pipeline.start(config)\n","\n","\n","\n","    try:\n","        while True:\n","            frames = pipeline.wait_for_frames()\n","            depth_frame = frames.get_depth_frame()\n","            color_frame = frames.get_color_frame()\n","            if not depth_frame or not color_frame:\n","                continue\n","\n","            # Convert images to numpy arrays\n","            depth_image = np.asanyarray(depth_frame.get_data())\n","            color_image = np.asanyarray(color_frame.get_data())\n","\n","            # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n","            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n","\n","             # Stack both images horizontally\n","            images = np.hstack((color_image, depth_colormap))\n","\n","            # Show images\n","            save_image = cv2.resize(color_image, (224, 224), interpolation=cv2.INTER_CUBIC)\n","            image.value = cv2.imencode('.jpg',save_image)[1].tobytes()\n","            #image.value = cv2.imencode('.jpg',images)[1].tobytes()\n","\n","    finally:\n","\n","        # Stop streaming\n","        pipeline.stop()\n","\n","thread =threading.Thread(target=rs_on)\n","thread.start()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWcS5_uL7aLy","colab_type":"text"},"source":["## Next\n","\n","Once you've collected enough data, we'll need to copy that data to our GPU desktop or cloud machine for training.  First, we can call the following *terminal* command to compress\n","our dataset folder into a single *zip* file.\n","\n","> The ! prefix indicates that we want to run the cell as a *shell* (or *terminal*) command.\n","\n","> The -r flag in the zip command below indicates *recursive* so that we include all nested files, the -q flag indicates *quiet* so that the zip command doesn't print any output"]},{"cell_type":"code","metadata":{"id":"jPMNHYkQ7aLz","colab_type":"code","colab":{}},"source":["!zip -r -q dataset.zip dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9nCsIWi7aL3","colab_type":"text"},"source":["You should see a file named ``dataset.zip`` in the Jupyter Lab file browser.  You should download the zip file using the Jupyter Lab file browser by right clicking and selecting ``Download``.\n","\n","Next, we'll need to upload this data to our GPU desktop or cloud machine (we refer to this as the *host*) to train the collision avoidance neural network.  We'll assume that you've set up your training\n","machine as described in the JetBot WiKi.  If you have, you can navigate to ``http://<host_ip_address>:8888`` to open up the Jupyter Lab environment running on the host.  The notebook you'll need to open there is called ``collision_avoidance/train_model.ipynb``.\n","\n","So head on over to your training machine and follow the instructions there!  Once your model is trained, we'll return to the robot Jupyter Lab enivornment to use the model for a live demo!"]}]}